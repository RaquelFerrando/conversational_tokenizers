{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "#parameters for experiment:\n",
    "model = \"mistralai/Mistral-7B-v0.1\"\n",
    "#\"meta-llama/Llama-3.1-8B\"\n",
    "#\"mistralai/Mistral-7B-v0.3\"\n",
    "#\"01-ai/Yi-1.5-9B-Chat\"\n",
    "#\"google/gemma-2-9b\"\n",
    "#\"deepseek-ai/DeepSeek-R1\"\n",
    "\n",
    "only_input = False #True to train only with human-generated text\n",
    "split_by_lang = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "#choose the train and test splits\n",
    "if split_by_lang:\n",
    "    train_corpus = Dataset.from_parquet(\"train_languagesplit.parquet\") #generate with create_corpus.ipynb\n",
    "    test_corpus = Dataset.from_parquet(\"test_languagesplit.parquet\") #generate with create_corpus.ipynb\n",
    "else:\n",
    "    train_corpus = Dataset.from_parquet(\"train_randomsplit.parquet\") #generate with create_corpus.ipynb\n",
    "    test_corpus = Dataset.from_parquet(\"test_randomsplit.parquet\") #generate with create_corpus.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "if only_input:\n",
    "    column = \"clean_input\" #column with only \"user\" texts\n",
    "else:\n",
    "    column = \"clean_conversation\" #column with \"user\" and \"assistant\" texts\n",
    "\n",
    "#to avoid loading all the texts into memory\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc = \"Traning Progress\"):\n",
    "        yield dataset[i : i + batch_size][column]\n",
    "\n",
    "\n",
    "#we always test the tokenization in both \"user\" and \"assistant\" texts\n",
    "def  get_test_corpus(test_dataset):\n",
    "    column = \"clean_conversation\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_count_from_generator(generator, tokenizer):\n",
    "    total_tokens = 0\n",
    "    for fragment_list in generator: \n",
    "        batch_tokenized = tokenizer(\n",
    "            fragment_list,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        total_tokens += sum(len(ids) for ids in batch_tokenized[\"input_ids\"]) #numerical identifiers of the tokens\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(base_name):\n",
    "\n",
    "    # Añade etiquetas al nombre base dependiendo de las flags\n",
    "    if only_input:\n",
    "        base_name += \"_only_input\"\n",
    "    if split_by_lang:\n",
    "        base_name += \"_split_by_lang\"\n",
    "\n",
    "    # Añade la extensión al archivo\n",
    "    base_name += \".csv\"\n",
    "    return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import csv\n",
    "%pip install --upgrade transformers accelerate\n",
    "\n",
    "print(f\"Processing: {model}\")\n",
    "\n",
    "#Loads the tokenizer from the model\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "print(\"Original tokenizer loaded\")\n",
    "\n",
    "#train the new tokenizer using the iterator from the training corpus\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(batch_iterator(train_corpus), old_tokenizer.vocab_size, length=len(train_corpus))\n",
    "print(\"Retrained tokenizer completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the number of tokens for the test corpus for the retrained tokenizer\n",
    "tokens_new = tokenize_and_count_from_generator(get_test_corpus(test_corpus), tokenizer)\n",
    "print(\"tokens_new obtained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the number of tokens for the test corpus for the original tokenizer\n",
    "tokens_old = tokenize_and_count_from_generator(get_test_corpus(test_corpus), old_tokenizer)\n",
    "print(\"tokens_old obtained\")\n",
    "# Computes the gain achieved by the retraining\n",
    "gain = (1 - tokens_new / tokens_old) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Print the details of the model and token counts\n",
    "print(f\"Model: {model}, Old tokens: {tokens_old}, New tokens: {tokens_new}, Gain: {gain:.2f}%\")\n",
    "\n",
    "# Create the CSV file name\n",
    "csv_file = create_filename(\"conversational_tokenizers\")\n",
    "\n",
    "# Check if the file already exists\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# Open the file in \"append\" mode if it exists, or \"write\" mode if it doesn't\n",
    "with open(csv_file, mode=\"a\" if file_exists else \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"model\", \"tokens_old\", \"tokens_new\", \"gain\"])\n",
    "    \n",
    "    # If the file doesn't exist, write the header\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # Directly write the single result row\n",
    "    writer.writerow({\n",
    "        \"model\": model,\n",
    "        \"tokens_old\": tokens_old,\n",
    "        \"tokens_new\": tokens_new,\n",
    "        \"gain\": round(gain, 2)  # Round the gain to 2 decimal places\n",
    "    })\n",
    "\n",
    "print(f\"Results saved in {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
