{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import csv\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "#ANALYSIS AND COMPARISON OF THE TOKEN VOCABULARIES \n",
    "\n",
    "#Hugging Face login\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "model = \"google/gemma-2-9b\" #a different model can be processed in each execution of the code, these ones or others:\n",
    "#\"mistralai/Mistral-7B-v0.1\"\n",
    "#\"meta-llama/Llama-3.1-8B\"\n",
    "#\"google/gemma-2-9b\"\n",
    "#\"deepseek-ai/DeepSeek-R1\"\n",
    "#\"bigscience/bloom\"\n",
    "#\"microsoft/phi\"\n",
    "\n",
    "model_name = model.split(\"/\")[-1] #to extract the local tokenizers from their folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenizers\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True) #original tokenizer from Hugging Face\n",
    "tokenizer_input = AutoTokenizer.from_pretrained(f\"retrained_conversational_tokenizers/tokenizer_{model_name}_input\")\n",
    "tokenizer_output = AutoTokenizer.from_pretrained(f\"retrained_conversational_tokenizers/tokenizer_{model_name}_output\")\n",
    "tokenizer_conversation = AutoTokenizer.from_pretrained(f\"retrained_conversational_tokenizers/tokenizer_{model_name}_conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computation of the percentage of common tokens between the different tokenizers of the same model:\n",
    "\n",
    "#create output folder (if it does not exist yet)\n",
    "output_dir = \"comparison_vocabularies\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing model: {model_name}\")\n",
    "\n",
    "#dictionary with vocabularies\n",
    "tokenizers = {\n",
    "    \"original\": set(original_tokenizer.get_vocab().keys()),\n",
    "    \"input\": set(tokenizer_input.get_vocab().keys()),\n",
    "    \"output\": set(tokenizer_output.get_vocab().keys()),\n",
    "    \"conversation\": set(tokenizer_conversation.get_vocab().keys()),\n",
    "}\n",
    "\n",
    "tokenizer_names = list(tokenizers.keys())\n",
    "num_tokens = len(tokenizers[\"original\"])  #take original as reference (all should have the same, or almost)\n",
    "print(f\"{model_name}: {num_tokens} tokens in the original vocabulary\")\n",
    "\n",
    "#prepare results\n",
    "results = []\n",
    "for name1, name2 in combinations(tokenizer_names, 2):\n",
    "    vocab1 = tokenizers[name1]\n",
    "    vocab2 = tokenizers[name2]\n",
    "\n",
    "    #intersection and common percentage (with respect to original)\n",
    "    common_tokens = vocab1 & vocab2\n",
    "    percent_common = len(common_tokens) / num_tokens * 100\n",
    "    results.append([name1, name2, len(common_tokens), f\"{percent_common:.2f}\"])\n",
    "\n",
    "#save results in csv\n",
    "csv_filename = os.path.join(output_dir, f\"tokenizer_comparison_{model_name}.csv\") #create a csv for each model inside the \"comparison_vocabularies\" folder\n",
    "file_exists = os.path.isfile(csv_filename)\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"tokenizer 1\", \"tokenizer 2\", \"number shared tokens\", \"% common\"])\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test split of the conversational dataset\n",
    "test_corpus = Dataset.from_parquet(\"test_randomsplit.parquet\") #generated with create_corpus.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to test input and output separately:\n",
    "def  get_test_corpus_input(test_dataset):\n",
    "    column = \"clean_input\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "\n",
    "def  get_test_corpus_output(test_dataset):\n",
    "    column = \"clean_output\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "    \n",
    "#to tokenize text from a generator and count the number of times that each token appears \n",
    "def tokenize_and_analyze(generator, tokenizer):\n",
    "    token_counts = Counter() #dictionary to save the number of times each token appears\n",
    "    for fragment_list in generator:\n",
    "        batch_tokenized = tokenizer(\n",
    "            fragment_list,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        for token_ids in batch_tokenized[\"input_ids\"]:\n",
    "            tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            token_counts.update(tokens) \n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of tokenizers\n",
    "tokenizers = { #imported previously\n",
    "    \"original\": original_tokenizer,\n",
    "    \"input\": tokenizer_input,\n",
    "    \"output\": tokenizer_output,\n",
    "    \"conversation\": tokenizer_conversation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the test corpus and save the 10,000 most common tokens for each tokenizer:\n",
    "\n",
    "#create output folder (if it does not exist yet)\n",
    "output_folder = f\"token_frequencies/{model_name}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#helper function to analyze, save and compare\n",
    "def process_corpus(get_corpus_fn, suffix):\n",
    "    #get token counts\n",
    "    token_counts_all = {\n",
    "        name: tokenize_and_analyze(get_corpus_fn(test_corpus), tokenizer)\n",
    "        for name, tokenizer in tokenizers.items()\n",
    "    }\n",
    "    #save the 10,000 most frequent tokens to a csv for each tokenizer\n",
    "    for name, counts in token_counts_all.items():\n",
    "        top_10000 = counts.most_common(10000) \n",
    "        csv_path = os.path.join(output_folder, f\"tokenizer_{name}_top_tokens_in_{suffix}.csv\") #generates, for each of the 4 tokenizers, to csv files (input and output)\n",
    "        with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"token\", \"frequency\"]) \n",
    "            writer.writerows(top_10000)\n",
    "        print(f\"results exported to: {csv_path}\") \n",
    "\n",
    "    #comparison: original vs input, output, conversation\n",
    "    N_values = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "    original_counts = token_counts_all[\"original\"]\n",
    "    #show preliminary results, but does not save them (will be saved later, after computing also the top tokens for complete conversations)\n",
    "    for name in [\"conversation\", \"input\", \"output\"]: \n",
    "        print(f\"\\noriginal-{name.upper()} ({suffix})\") #suffix indicates if it is the input or the output what has been tokenized\n",
    "        other_counts = token_counts_all[name]\n",
    "        for N in N_values:\n",
    "            top_original = set([token for token, _ in original_counts.most_common(N)])\n",
    "            top_other = set([token for token, _ in other_counts.most_common(N)])\n",
    "            common = top_original.intersection(top_other)\n",
    "            percent_common = len(common) / N * 100\n",
    "            print(f\"top {N}: {percent_common:.2f}%\")\n",
    "\n",
    "#run analysis for input and output\n",
    "process_corpus(get_test_corpus_input, \"input\")\n",
    "process_corpus(get_test_corpus_output, \"output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a third file of top tokens for each tokenizer: the one that combines input and output to extract the most common tokens in conversations\n",
    "\n",
    "#folder where the csv files are located\n",
    "output_folder = f\"token_frequencies/{model_name}\"\n",
    "tokenizer_names = [\"original\", \"input\", \"output\", \"conversation\"]\n",
    "\n",
    "#to extract the token counts \n",
    "def load_token_counts(filepath): \n",
    "    counts = Counter() #a dictionary\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            token = row[\"Token\"]\n",
    "            freq = int(row[\"Frequency\"])\n",
    "            counts[token] += freq #counts[token] = freq could be used as well because each tokens appears only once with the total frequency already\n",
    "    return counts \n",
    "\n",
    "#to add the frequencies in input and output and extract the ones in conversation\n",
    "def combine_input_output_to_conversation():\n",
    "    for name in tokenizer_names:\n",
    "        try:\n",
    "            #load the two files of the tokenizer that is being processed\n",
    "            path_input = os.path.join(output_folder, f\"tokenizer_{name}_top_tokens_in_input.csv\") \n",
    "            path_output = os.path.join(output_folder, f\"tokenizer_{name}_top_tokens_in_output.csv\")\n",
    "            \n",
    "            #dictionary with token counts of each file\n",
    "            counts_input = load_token_counts(path_input) \n",
    "            counts_output = load_token_counts(path_output)\n",
    "            \n",
    "            combined = counts_input + counts_output #combines the dictionaries and calculates the total counts\n",
    "            top_combined = combined.most_common(10000)\n",
    "            \n",
    "            path_convo = os.path.join(output_folder, f\"tokenizer_{name}_top_tokens_in_conversation.csv\") #new file generated for each tokenizer processed\n",
    "            with open(path_convo, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"Token\", \"Frequency\"])\n",
    "                writer.writerows(top_combined)\n",
    "            \n",
    "            print(f\"Combined input+output:  {path_convo}\")\n",
    "        \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Files missing for {name}: {e}\")\n",
    "\n",
    "#Execute\n",
    "combine_input_output_to_conversation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the csv files generated with the top 10,000 tokens of each tokenizer: comparison between the top tokens of the different tokenizers\n",
    "\n",
    "#parameters\n",
    "comparisons = [(\"original\", \"input\"), (\"original\", \"output\"), (\"original\", \"conversation\")]\n",
    "N_values = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "tokenizer_names = [\"original\", \"input\", \"output\", \"conversation\"]\n",
    "\n",
    "#load tokens from csv file\n",
    "def load_top_tokens(filepath, max_tokens=10000):\n",
    "    tokens = []\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            tokens.append(row[\"Token\"])\n",
    "            if len(tokens) >= max_tokens:\n",
    "                break\n",
    "    return tokens\n",
    "\n",
    "#analyze token overlap from saved csv files\n",
    "def analyze_overlap_from_csv(output_folder, suffix):\n",
    "    #load tokens from each tokenizer file\n",
    "    top_tokens = {}\n",
    "    for name in tokenizer_names:\n",
    "        path = os.path.join(output_folder, f\"tokenizer_{name}_top_tokens_in_{suffix}.csv\")\n",
    "        top_tokens[name] = load_top_tokens(path, max_tokens=max(N_values))\n",
    "\n",
    "    #compare and store results\n",
    "    results = []\n",
    "\n",
    "    for a, b in comparisons:\n",
    "        row = [f\"{a}-{b}\"]\n",
    "        tokens_a = top_tokens[a]\n",
    "        tokens_b = top_tokens[b]\n",
    "        for N in N_values:\n",
    "            top_a = set(tokens_a[:N])\n",
    "            top_b = set(tokens_b[:N])\n",
    "            common = top_a.intersection(top_b)\n",
    "            percent_common = len(common) / N * 100\n",
    "            row.append(f\"{percent_common:.2f}\")\n",
    "        results.append(row)\n",
    "\n",
    "    #save results to csv\n",
    "    output_csv = os.path.join(output_folder, f\"comparison_overlap_{suffix}.csv\") #three new csv files for each model: input, output and conversation\n",
    "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"comparison\"] + [f\"top {n}\" for n in N_values])\n",
    "        writer.writerows(results)\n",
    "    print(f\"Comparison saved to: {output_csv}\")\n",
    "\n",
    "#run for input, output and conversation\n",
    "#for each model, the csv files generated previously with the most common tokens are in the folder \"token_frequencies/{model_name}\"\n",
    "analyze_overlap_from_csv(f\"token_frequencies/{model_name}\", \"input\") \n",
    "analyze_overlap_from_csv(f\"token_frequencies/{model_name}\", \"output\")\n",
    "analyze_overlap_from_csv(f\"token_frequencies/{model_name}\", \"conversation\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
