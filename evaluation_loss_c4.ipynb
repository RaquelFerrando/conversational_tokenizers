{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d533c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "#EVALUATE THE LOSS OF THE RETRAINED TOKENIZERS IN THE C4 CORPUS\n",
    "\n",
    "#Hugging Face login\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "#models to evaluate\n",
    "hf_models = [\n",
    "    \"deepseek-ai/DeepSeek-R1\",\n",
    "   \"meta-llama/Llama-3.1-8B\",\n",
    "    \"google/gemma-2-9b\",\n",
    "   \"mistralai/Mistral-7B-v0.1\",\n",
    "   \"bigscience/bloom\",\n",
    "   \"microsoft/phi-4\"\n",
    "] #a loop is going to be used to obtain all the results in one execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626248b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in streaming mode\n",
    "dataset = load_dataset(\"allenai/c4\", \"en\", streaming=True) #english conversations (as in the first evaluation)\n",
    "validation_dataset = dataset[\"validation\"] #the train split is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to count the number of tokens in a dataset\n",
    "def tokenize_and_count(dataset, tokenizer, tokenizer_name):\n",
    "    total_tokens = 0\n",
    "    for example in tqdm(dataset, desc=f\"Tokenizing C4 with {tokenizer_name}\"):\n",
    "        text = example[\"text\"]\n",
    "        tokens = tokenizer(text, padding=False, truncation=False)[\"input_ids\"]\n",
    "        total_tokens += len(tokens)\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"loss_c4_retrained.csv\" #file where the results are going to be saved\n",
    "fieldnames = [ #for the csv file with the results\n",
    "    \"model\", \n",
    "    \"tokens_original\",\n",
    "    \"tokens_retrained_conversation\", \"gain_conversation\",\n",
    "    \"tokens_retrained_input\", \"gain_input\",\n",
    "    \"tokens_retrained_output\", \"gain_output\"\n",
    "]\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader() #write the fieldnames\n",
    "\n",
    "    for model in hf_models:\n",
    "        print(f\"Processing model: {model}\")\n",
    "        model_name = model.split(\"/\")[-1]\n",
    "\n",
    "        try:\n",
    "            #load tokenizers\n",
    "            original_tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "            tokenizer_input = AutoTokenizer.from_pretrained(f\"conversational_tokenizers/tokenizer_{model_name}_input\")\n",
    "            tokenizer_output = AutoTokenizer.from_pretrained(f\"conversational_tokenizers/tokenizer_{model_name}_output\")\n",
    "            tokenizer_conversation = AutoTokenizer.from_pretrained(f\"conversational_tokenizers/tokenizer_{model_name}_conversation\")\n",
    "\n",
    "            #token counts\n",
    "            tokens_original = tokenize_and_count(validation_dataset, original_tokenizer, model_name + \" (original)\")\n",
    "            tokens_retrained_conversation = tokenize_and_count(validation_dataset, tokenizer_conversation, model_name + \" (conversation)\")\n",
    "            tokens_retrained_input = tokenize_and_count(validation_dataset, tokenizer_input, model_name + \" (input)\")\n",
    "            tokens_retrained_output = tokenize_and_count(validation_dataset, tokenizer_output, model_name + \" (output)\")\n",
    "\n",
    "            #compute gains\n",
    "            gain_conversation = (1-tokens_retrained_conversation/tokens_original)*100\n",
    "            gain_input = (1-tokens_retrained_input/tokens_original)*100\n",
    "            gain_output = (1-tokens_retrained_output/tokens_original)*100\n",
    "\n",
    "            #add to csv\n",
    "            writer.writerow({\n",
    "                \"model\": model_name,\n",
    "                \"tokens_original\": tokens_original,\n",
    "                \"tokens_retrained_conversation\": tokens_retrained_conversation,\n",
    "                \"gain_conversation\": round(gain_conversation, 2),\n",
    "                \"tokens_retrained_input\": tokens_retrained_input,\n",
    "                \"gain_input\": round(gain_input, 2),\n",
    "                \"tokens_retrained_output\": tokens_retrained_output,\n",
    "                \"gain_output\": round(gain_output, 2)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
