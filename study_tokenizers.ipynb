{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981eedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#ANALYSIS OF THE HUGGING FACE TOKENIZER CONFIGURATION FOR THE SELECTED MODELS\n",
    "\n",
    "#Hugging Face login\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "#\"mistralai/Mistral-7B-v0.1\"\n",
    "#\"meta-llama/Llama-3.1-8B\"\n",
    "#\"google/gemma-2-9b\"\n",
    "#\"deepseek-ai/DeepSeek-R1\"\n",
    "#\"bigscience/bloom\"\n",
    "#\"microsoft/phi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to extract the fast tokenizer backend (if available)\n",
    "if hasattr(tokenizer, \"backend_tokenizer\"):\n",
    "    backend = tokenizer.backend\n",
    "    model_type = type(backend.model)\n",
    "    pretokenizer_type = type(backend.pre_tokenizer)\n",
    "    postprocessor_type = type(backend.post_processor)\n",
    "else:\n",
    "    backend = None\n",
    "    model_type = pretokenizer_type = postprocessor_type = None\n",
    "\n",
    "#general info\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer).__name__}\")\n",
    "print(f\"Case-sensitive: {not getattr(tokenizer, 'do_lower_case', False)}\")\n",
    "\n",
    "#backend tokenizer details\n",
    "print(\"Tokenizer backend:\")\n",
    "print(f\"Model: {model_type}\")\n",
    "print(f\"Pretokenizer: {pretokenizer_type}\")\n",
    "print(f\"Postprocessor: {postprocessor_type}\")\n",
    "\n",
    "#special tokens\n",
    "print(\"special tokens:\")\n",
    "for name, token in tokenizer.special_tokens_map.items():\n",
    "    print(f\"  {name}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb39dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dig deeper into the pretokenizer, tokenizer and normalizer:\n",
    "\n",
    "#Normalizer:\n",
    "normalizer = backend.normalizer\n",
    "print(\"Normalizer:\")\n",
    "print(repr(normalizer), \"\\n\")\n",
    "\n",
    "#Pretokenizer:\n",
    "pretokenizer = backend.pre_tokenizer\n",
    "print(\"PreTokenizer:\")\n",
    "print(repr(pretokenizer), \"\\n\")\n",
    "\n",
    "#Postprocessor:\n",
    "postprocessor = backend.post_processor\n",
    "print(\"PostProcessor:\")\n",
    "print(repr(postprocessor), \"\\n\")\n",
    "\n",
    "#Model:\n",
    "model = backend.model\n",
    "print(\"Model (BPE, WordPiece, etc):\")\n",
    "print(repr(model), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff046104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine the vocabulary:\n",
    "\n",
    "#get the vocabulary as a dictionary {token: id}\n",
    "vocab_dict = tokenizer.get_vocab()\n",
    "\n",
    "#sort tokens by their id\n",
    "vocab_list = sorted(vocab_dict.items(), key=lambda x: x[1])  # list of (token, id)\n",
    "\n",
    "#save to a .txt file\n",
    "with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token, token_id in vocab_list:\n",
    "        f.write(f\"{token}\\t{token_id}\\n\")\n",
    "\n",
    "print(f\"Vocabulary extracted: {len(vocab_list)} tokens saved in vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416882ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare two tokenizers (to check if different versions of the model share the same tokenizer):\n",
    "\n",
    "#load the two tokenizers\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3\")\n",
    "\n",
    "#list of test sentences\n",
    "test_sentences = [\n",
    "    \"Hello world!\",\n",
    "    \"This is a test.\",\n",
    "    \"Tokenization can be tricky...\",\n",
    "    \"Let's compare two tokenizers.\",\n",
    "    \"üòä Unicode and symbols #hashtag\",\n",
    "    \"Don't stop believing.\",\n",
    "    \"test@example.com.\",\n",
    "    \"C'est la vie ‚Äî that‚Äôs life.\",\n",
    "    \"¬øC√≥mo est√°s? ¬°Muy bien!\",\n",
    "    \"‰∏≠ÊñáÂàÜËØçÊµãËØï\",\n",
    "    \"I have 2 dogs and 3 cats.\",\n",
    "    \"Newlines\\nshould\\nalso\\nbe\\ntested.\",\n",
    "    \"He said, 'Hello!' and left.\",\n",
    "    \"Hyphenated-words can be tricky.\",\n",
    "    \"1234567890 numbers test\",\n",
    "    \"a\" * 300,  # very long word\n",
    "    \"Mix of CAPS and lowercase.\",\n",
    "    \"Emojis ü§îüî•üöÄ\",\n",
    "    \"Some_math_symbols + ‚àí √ó √∑ = ‚â†\",\n",
    "    \"URLs like https://huggingface.co\",\n",
    "    \"file_name_with_underscores.py\"\n",
    "]\n",
    "\n",
    "#compare tokenization results\n",
    "for sentence in test_sentences:\n",
    "    tokens1 = tokenizer1.tokenize(sentence)\n",
    "    tokens2 = tokenizer2.tokenize(sentence)\n",
    "    if tokens1 != tokens2:\n",
    "        print(f\"Difference found in: '{sentence}'\")\n",
    "        print(f\"Tokenizer 1: {tokens1}\")\n",
    "        print(f\"Tokenizer 2: {tokens2}\\n\")\n",
    "    else:\n",
    "        print(f\"Same result for: '{sentence}'\")\n",
    "\n",
    "#check if all tokenizations are identical\n",
    "all_equal = all(tokenizer1.tokenize(s) == tokenizer2.tokenize(s) for s in test_sentences)\n",
    "print(\"\\nAre all results the same?:\", all_equal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
