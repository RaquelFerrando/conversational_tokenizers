{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "#parameters for experiment:\n",
    "model = \"google/gemma-2-9b\"\n",
    "#\"mistralai/Mistral-7B-v0.1\"\n",
    "#\"mistralai/Mistral-7B-v0.1\"\n",
    "#\"meta-llama/Llama-3.1-8B\"\n",
    "#\"mistralai/Mistral-7B-v0.3\"\n",
    "#\"01-ai/Yi-1.5-9B-Chat\"\n",
    "#\"google/gemma-2-9b\"\n",
    "#\"deepseek-ai/DeepSeek-R1\"\n",
    "\n",
    "mode_training = \"output\" #\"output\", \"conversation\"\n",
    "split_by_lang = False\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "#choose the train and test splits\n",
    "if split_by_lang:\n",
    "    train_corpus = Dataset.from_parquet(\"train_languagesplit.parquet\") #generate with create_corpus.ipynb\n",
    "    test_corpus = Dataset.from_parquet(\"test_languagesplit.parquet\") #generate with create_corpus.ipynb\n",
    "else:\n",
    "    train_corpus = Dataset.from_parquet(\"train_randomsplit.parquet\") #generate with create_corpus.ipynb\n",
    "    test_corpus = Dataset.from_parquet(\"test_randomsplit.parquet\") #generate with create_corpus.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#to avoid loading all the texts into memory\n",
    "def batch_iterator(dataset, batch_size=1000, mode = \"conversation\"): #used for training\n",
    "    column_map = {\n",
    "    \"input\": \"clean_input\",\n",
    "    \"output\": \"clean_output\",  # Aseg√∫rate de que esta columna existe\n",
    "    \"conversation\": \"clean_conversation\"\n",
    "    }\n",
    "    column = column_map[mode]\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc = \"Traning Progress\"):\n",
    "        yield dataset[i : i + batch_size][column]\n",
    "\n",
    "\n",
    "\n",
    "#to test the number of tokens generated for input and output separately:\n",
    "\n",
    "def  get_test_corpus_input(test_dataset):\n",
    "    column = \"clean_input\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "\n",
    "def  get_test_corpus_output(test_dataset):\n",
    "    column = \"clean_output\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_count_from_generator(generator, tokenizer):\n",
    "    total_tokens = 0\n",
    "    for fragment_list in generator: \n",
    "        batch_tokenized = tokenizer(\n",
    "            fragment_list,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        total_tokens += sum(len(ids) for ids in batch_tokenized[\"input_ids\"]) #numerical identifiers of the tokens\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(base_name, mode, split_by_lang):\n",
    "    mode_map = {\n",
    "        \"input\": \"only_input\",\n",
    "        \"output\": \"only_output\",\n",
    "        \"conversation\": \"conversation\"\n",
    "    }\n",
    "\n",
    "    base_name += f\"_{mode_map[mode]}\"\n",
    "    \n",
    "    if split_by_lang:\n",
    "        base_name += \"_split_by_lang\"\n",
    "\n",
    "    base_name += \".csv\"\n",
    "    return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %install --upgrade transformers accelerate\n",
    "from transformers import AutoTokenizer\n",
    "import csv\n",
    "\n",
    "\n",
    "print(f\"Processing: {model}\")\n",
    "\n",
    "#Loads the tokenizer from the model\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "print(\"Original tokenizer loaded\")\n",
    "\n",
    "#train the new tokenizer using the iterator from the training corpus\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(batch_iterator(train_corpus, batch_size, mode_training), old_tokenizer.vocab_size, length=len(train_corpus))\n",
    "print(\"Retrained tokenizer completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the number of tokens for the input of the test corpus for the retrained tokenizer\n",
    "tokens_new_input = tokenize_and_count_from_generator(get_test_corpus_input(test_corpus), tokenizer)\n",
    "print(\"tokens_new_input obtained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the number of tokens for the input of the test corpus for the original tokenizer\n",
    "tokens_old_input = tokenize_and_count_from_generator(get_test_corpus_input(test_corpus), old_tokenizer)\n",
    "print(\"tokens_old_input obtained\")\n",
    "# Computes the gain achieved by the retraining for the input (user) texts\n",
    "gain_input = (1 - tokens_new_input / tokens_old_input) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the number of tokens for the output of the test corpus for the retrained tokenizer\n",
    "tokens_new_output = tokenize_and_count_from_generator(get_test_corpus_output(test_corpus), tokenizer)\n",
    "print(\"tokens_new_output obtained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the number of tokens for the input of the test corpus for the original tokenizer\n",
    "tokens_old_output = tokenize_and_count_from_generator(get_test_corpus_output(test_corpus), old_tokenizer)\n",
    "print(\"tokens_old_output obtained\")\n",
    "# Computes the gain achieved by the retraining for the output (assistant) texts\n",
    "gain_output = (1 - tokens_new_output / tokens_old_output) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Print the details of the model and token counts\n",
    "\n",
    "print(f\"Modelo: {model}, Tokens antiguos input: {tokens_old_input}, Tokens nuevos input: {tokens_new_input}, Ganancia input: {gain_input:.2f}%, Tokens antiguos output: {tokens_old_output}, Tokens nuevos output: {tokens_new_output}, Ganancia output: {gain_output:.2f}% \")\n",
    "\n",
    "# Create the CSV file name\n",
    "csv_file = create_filename(\"conversational_tokenizers\", mode_training, split_by_lang)\n",
    "\n",
    "# Check if the file already exists\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# Open the file in \"append\" mode if it exists, or \"write\" mode if it doesn't\n",
    "with open(csv_file, mode=\"a\" if file_exists else \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        \"model\", \n",
    "        \"tokens_old_input\", \"tokens_new_input\", \"gain_input\",\n",
    "        \"tokens_old_output\", \"tokens_new_output\", \"gain_output\"\n",
    "    ])\n",
    "    # If the file doesn't exist, write the header\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # Directly write the single result row\n",
    "    writer.writerow({\n",
    "        \"model\": model,\n",
    "        \"tokens_old_input\": tokens_old_input,\n",
    "        \"tokens_new_input\": tokens_new_input,\n",
    "        \"gain_input\": round(gain_input, 2),  # Round the gain to 2 decimal places\n",
    "        \"tokens_old_output\": tokens_old_output,\n",
    "        \"tokens_new_output\": tokens_new_output,\n",
    "        \"gain_output\": round(gain_output, 2),  # Round the gain to 2 decimal places        \n",
    "    })\n",
    "\n",
    "print(f\"Results saved in {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
