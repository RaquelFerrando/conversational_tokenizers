{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install pyarrow\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "dataset = load_dataset('lmsys/lmsys-chat-1m', trust_remote_code=True)\n",
    "#the dataset only has \"train\"\n",
    "dataset[\"train\"].to_parquet(\"train_dataset_lmsys.parquet\") #code to generate train_dataset_lmsys.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_lang = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "from collections import defaultdict\n",
    "\n",
    "#to avoid loading all the conversations into memory\n",
    "def batch_group(group, batch_size=10000):\n",
    "    for i in range(0, len(group), batch_size):\n",
    "        yield group[i:i + batch_size] #returns a batch\n",
    "\n",
    "#for the case \"split_by_lang = True\". Divides the dataset in test and train splits according to the language of the conversation\n",
    "#since the number of conversations is very big, this should not have a major effect on the performance \n",
    "def split_dataset_by_language(dataset, test_size=0.2, seed=42, batch_size=10000):\n",
    "    split_datasets = {'train': [], 'test': []}\n",
    "    \n",
    "    # group conversations by language\n",
    "    grouped_by_language = defaultdict(list)\n",
    "    for entry in dataset:\n",
    "        language = entry['language']\n",
    "        grouped_by_language[language].append(entry) \n",
    "    \n",
    "    #train and test splits for each language\n",
    "    for lang, group in grouped_by_language.items():\n",
    "        if len(group) > batch_size:\n",
    "            for batch in batch_group(group, batch_size):\n",
    "                batch_dict = {key: [entry[key] for entry in batch] for key in batch[0].keys()}\n",
    "                batch_dataset = Dataset.from_dict(batch_dict)\n",
    "                split = batch_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "                split_datasets['train'].append(split['train'])\n",
    "                split_datasets['test'].append(split['test'])\n",
    "        else:\n",
    "            group_dict = {key: [entry[key] for entry in group] for key in group[0].keys()}\n",
    "            group_dataset = Dataset.from_dict(group_dict)\n",
    "            if len(group) < 2: #languages with only 1 conversation: directly to train split\n",
    "                split_datasets['train'].append(group_dataset)\n",
    "            else:\n",
    "                split = group_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "                split_datasets['train'].append(split['train'])\n",
    "                split_datasets['test'].append(split['test'])\n",
    "    \n",
    "    # combine all the subsets of the different languages\n",
    "    combined_train = concatenate_datasets(split_datasets['train'])\n",
    "    combined_test = concatenate_datasets(split_datasets['test'])\n",
    "    \n",
    "    return combined_train, combined_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "from collections import defaultdict\n",
    "# load the dataset from the Parquet file\n",
    "train_dataset = Dataset.from_parquet(\"train_dataset_lmsys.parquet\")\n",
    "\n",
    "if split_by_lang:\n",
    "    \n",
    "    #GENERATE CORPUS: SPLIT BY LANGUAGE\n",
    "    \n",
    "    # obtain train and test splits\n",
    "    combined_train, combined_test = split_dataset_by_language(train_dataset, batch_size=5000)\n",
    "\n",
    "    # save results in parquet format\n",
    "    combined_train.to_parquet(\"train_combined_splitbylanguage.parquet\")\n",
    "    combined_test.to_parquet(\"test_combined_splitbylanguage.parquet\") \n",
    "\n",
    "\n",
    "    print(f\"Train set size: {len(combined_train)}\")\n",
    "    print(f\"Test set size: {len(combined_test)}\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    #GENERATE CORPUS: RANDOM SPLIT\n",
    "    # 80% train 20% test\n",
    "    split = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "    # obtain train and test splits\n",
    "    combined_train = split['train']\n",
    "    combined_test = split['test']\n",
    "\n",
    "    # save results in parquet format\n",
    "    combined_train.to_parquet(\"train_combined_random.parquet\")\n",
    "    combined_test.to_parquet(\"test_combined_random.parquet\")\n",
    "\n",
    "    print(f\"Train set size: {len(combined_train)}\")\n",
    "    print(f\"Test set size: {len(combined_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def add_clean_columns(dataset):\n",
    "    \"\"\"\n",
    "    Adds three new columns to the dataset:\n",
    "    - 'clean_input': Contains only the texts from the 'user' role in each conversation.\n",
    "    - 'clean_conversation': Contains the texts from both 'user' and 'assistant', merged into a single string.\n",
    "    - 'clean_assistant': Contains only the texts from the 'assistant' role in each conversation.\n",
    "    \"\"\"\n",
    "    clean_inputs = [] \n",
    "    clean_conversations = []\n",
    "    clean_outputs = []\n",
    "    \n",
    "    # Iterate through each conversation and extract texts from 'user' and 'assistant'\n",
    "    for conversation in dataset[\"conversation\"]:\n",
    "        user_texts = [entry[\"content\"] for entry in conversation if entry[\"role\"] == \"user\"]\n",
    "        assistant_texts = [entry[\"content\"] for entry in conversation if entry[\"role\"] == \"assistant\"]\n",
    "        \n",
    "        clean_inputs.append(\" \".join(user_texts))\n",
    "        clean_outputs.append(\" \".join(assistant_texts))\n",
    "        clean_conversations.append(\" \".join(user_texts + assistant_texts))\n",
    "    \n",
    "    # Add the new columns to the dataset\n",
    "    dataset = dataset.add_column(\"clean_input\", clean_inputs)\n",
    "    dataset = dataset.add_column(\"clean_conversation\", clean_conversations)\n",
    "    dataset = dataset.add_column(\"clean_output\", clean_outputs)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Add the columns to the previously generated splits\n",
    "combined_train_clean = add_clean_columns(combined_train)\n",
    "combined_test_clean = add_clean_columns(combined_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results\n",
    "if split_by_lang:\n",
    "    combined_train_clean.to_parquet(\"train_languagesplit.parquet\")\n",
    "    combined_test_clean.to_parquet(\"test_languagesplit.parquet\")\n",
    "    print(\"Language split train and test saved\")\n",
    "else:\n",
    "    combined_train_clean.to_parquet(\"train_randomsplit.parquet\")\n",
    "    combined_test_clean.to_parquet(\"test_randomsplit.parquet\")\n",
    "    print(\"Random split train and test saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
