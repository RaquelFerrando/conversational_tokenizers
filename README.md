# Is There a Case for Conversation-Optimized Tokenizers in Large Language Models?

## Project Description
This repository contains the code, data, and evaluation metrics supporting the study "Is There a Case for Conversation-Optimized Tokenizers in Large Language Models?". The project explores whether current tokenization strategies are optimal for conversational use cases, and how conversation-optimized tokenizers can impact model efficiency.
The study includes:

- Evaluation of tokenizer fertility on both a standard LLM training dataset and a conversational corpus.
- Experiments involving tokenizer retraining to enhance performance on conversational data.
- Assessment of the efficiency improvements achieved by the retrained tokenizers in dialogue scenarios.
- Analysis of tokenization losses introduced when applying tokenizers to LLM training datasets.
- Language-wise evaluation of tokenization efficiency.
- Comparison of vocabulary compositions across different tokenizers and models.


## Analysis and evaluation of current LLM tokenizers
- `study_tokenizers.ipynb`: Script to see the internal Hugging Face configuration of the tokenizers.
- `evaluation_fertility.ipynb`: Script where a fertility metric is calculated for a conversational corpus ([LMSYS Chat 1M](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)) and an LLM training dataset ([C4 AllenAI](https://huggingface.co/datasets/allenai/c4)). The results for the models that were analyzed in this study are saved in [`evaluation_fertility_lmsys.csv`](./evaluation_fertility_lmsys.csv) and  [`evaluation_fertility_c4_allenai.csv`](./evaluation_fertility_c4_allenai.csv).

## Retraining tokenizers of LLMs on a conversational dataset
- `create_corpus.ipynb`: Script to build the train and test splits of the conversational corpus [LMSYS Chat 1M](https://huggingface.co/datasets/lmsys/lmsys-chat-1m). 80% of the corpus is used to retrain the tokenizers, and the remaining 20% is used to evaluate their performance.
The original dataset is also extended with the following columns to facilitate model training by converting the `conversation` field (originally in JSON format) into plain text:
  - `clean_input`: contains the user prompts of the conversation.
  - `clean_output`: contains only the assistant's responses.
  - `clean_conversation`: contains the full conversation (prompts and responses) in plain text format.
- `conversational_tokenizers_gain.ipynb`: Script implementing the retraining of the tokenizers. The original configurations of the tokenizers are preserved for all models, except for Mistral and Gemma. In these two cases, using the default Hugging Face configurations during retraining resulted in tokens that span multiple words, which is undesirable for this analysis. The updated configurations for these models are available in [tokenizer_mod_gemma-2-9b](./tokenizer_mod_gemma-2-9b) and [tokenizer_mod_Mistral-7B-v0.1](./tokenizer_mod_Mistral-7B-v0.1)  The retraining of the tokenizer can be performed in three different ways:
  - Retrained using only the messages written by users (using `clean_input`).
  - Retrained using only the responses generated by assistants (using `clean_output`).
  - Retrained using the entire conversations (using `clean_conversation`).
  The retrained tokenizers were saved and are available for use in the folder [retrained_conversational_tokenizers](./retrained_conversational_tokenizers).
  After the retraining is completed, the test corpus is tokenized using both the original and the retrained tokenizer. The token reduction achieved by each new tokenizer compared to the original one is reported in the files [`conversational_tokenizers_only_input.csv`](./conversational_tokenizers_only_input.csv), [`conversational_tokenizers_only_output.csv`](./conversational_tokenizers_only_output.csv) and [`conversational_tokenizers_conversation.csv`](./conversational_tokenizers_conversation.csv).

## Evaluation of losses of conversation-optimized tokenizers on a LLM training dataset
- `evaluation_loss_c4.ipynb`: Script that evaluates whether the retrained tokenizers introduce any performance degradation compared to the original one when applied to a subset of the [C4 AllenAI](https://huggingface.co/datasets/allenai/c4) dataset.  
  The token reduction achieved by each new tokenizer is computed, using the same method as in `conversational_tokenizers_gain.ipynb`.  
  Results are provided in [`loss_c4_retrained.csv`](./loss_c4_retrained.csv), where a positive value indicates that the retrained tokenizer performs better (produces fewer tokens) than the original one.

## Language-wise evaluation of conversation-optimized tokenizers
- `evaluation_gain_per_language.ipynb`: Studies the gain in efficiency of the new tokenizers for every language in the dataset. The results for the three retrained tokenizers (per model) are available in the folder [gains_by_language](./gains_by_language). This script also generates the file [`latin_conversations.txt`](./latin_conversations.txt), which contains a sample of 100 conversations labeled as Latin. Upon inspection, it becomes clear that this label is inaccurate.
- `language_check_tags.ipynb`: Script that verifies the accuracy of the language tags in the [LMSYS Chat 1M](https://huggingface.co/datasets/lmsys/lmsys-chat-1m) dataset, by cross-checking them with GPT-4.1-mini on a subset of conversations. Evaluation results for the 20 most frequent languages in the dataset are available in [`gpt_language_identification_all.csv`](./gpt_language_identification_all.csv) and [`language_accuracy_metrics.csv`](./language_accuracy_metrics.csv).

## Analysis and comparison of the token vocabularies
- `evaluation_vocab.ipynb`: This notebook performs various analyses related to the vocabularies of different tokenizers:
  - Computes the percentage of shared tokens among different tokenizers of the same model. The results for the three retrained tokenizers (per model) are available in the [comparison_vocabularies](./comparison_vocabularies) folder.
  - Compares the top tokens of the retrained tokenizers with those of the original tokenizer.  Specifically, the top 10,000 tokens from the test split for each tokenizer were saved along with their frequencies, and then compared. The results are in the folder [token_frequencies](./token_frequencies).
