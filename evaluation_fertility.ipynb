{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#EVALUATE FERTILITY IN CONVERSATIONS AND LLM TRAINING DATASET\n",
    "\n",
    "#Hugging Face login\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "#models to evaluate:\n",
    "hf_models = [\n",
    "    \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    \"google/gemma-2-9b\",\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"bigscience/bloom\"\n",
    "]\n",
    "openai_models = [\"gpt-4\", \"gpt4-o\"]\n",
    "\n",
    "#corpus to evaluate:\n",
    "corpus = \"c4_allenai\" #\"lmsys\" to evaluate conversations, \"c4_allenai\" to evaluate training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in streaming mode\n",
    "if corpus == \"lmsys\":\n",
    "     dataset = load_dataset('lmsys/lmsys-chat-1m', trust_remote_code=True, streaming=True)\n",
    "elif corpus == \"c4_allenai\":\n",
    "     dataset = load_dataset(\"allenai/c4\", \"en\", streaming=True) #only english texts\n",
    "     validation_dataset = dataset[\"validation\"] #the train split is too big \n",
    "else:\n",
    "    raise ValueError(\"Name of dataset not recognised. Use 'lmsys' or 'c4_allenai'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to count number of words in a text\n",
    "def count_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text) #this was chosen instead of split() to focus on actual words (alphanumeric sequences), ignoring punctuation and symbols\n",
    "    return len(words)\n",
    "\n",
    "#count number of words in the selected dataset:\n",
    "if corpus == \"lmsys\":\n",
    "    #initialize counters: count separately the words in input and output texts\n",
    "    total_words_input = 0\n",
    "    total_words_output = 0\n",
    "\n",
    "    #word processing\n",
    "    for conversation in tqdm(dataset[\"train\"], desc=\"Processing English conversations\"):\n",
    "        if conversation.get(\"language\") == \"English\": #only english to compare fertility with c4_allenai\n",
    "            if \"conversation\" in conversation: #make sure that we can access this field of the dataset\n",
    "                user_texts = [entry[\"content\"] for entry in conversation[\"conversation\"] if entry[\"role\"] == \"user\"]\n",
    "                assistant_texts = [entry[\"content\"] for entry in conversation[\"conversation\"] if entry[\"role\"] == \"assistant\"]\n",
    "\n",
    "                total_words_input += sum(count_words(text) for text in user_texts)\n",
    "                total_words_output += sum(count_words(text) for text in assistant_texts)\n",
    "\n",
    "    print(f\"Total number of words in user texts (English): {total_words_input}\")\n",
    "    print(f\"Total number of words in assistant texts (English): {total_words_output}\")\n",
    "    \n",
    "elif corpus == \"c4_allenai\":\n",
    "    #initialize counter\n",
    "    total_words = 0\n",
    "\n",
    "    for example in tqdm(validation_dataset, desc=\"Processing C4\"):\n",
    "        if \"text\" in example:\n",
    "            total_words += count_words(example[\"text\"])\n",
    "\n",
    "    print(f\"Total number of words in the C4 corpus validation split (English): {total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fertility calculation for OpenAI tokenizers:\n",
    "for openai_model in openai_models: #repeat process for each model\n",
    "    encoder = tiktoken.encoding_for_model(openai_model) #load the tokenizer\n",
    "\n",
    "    if corpus == \"lmsys\": #if the selected corpus is the conversational one\n",
    "        #initialize counters\n",
    "        total_tokens_input = 0 \n",
    "        total_tokens_output = 0\n",
    "\n",
    "        for conversation in tqdm(dataset[\"train\"], desc=f\"Tokenizing LMSYS for {openai_model}\"):\n",
    "            if conversation.get(\"language\") == \"English\":\n",
    "                if \"conversation\" in conversation:\n",
    "                    user_texts = [entry[\"content\"] for entry in conversation[\"conversation\"] if entry[\"role\"] == \"user\"] #extract input texts\n",
    "                    assistant_texts = [entry[\"content\"] for entry in conversation[\"conversation\"] if entry[\"role\"] == \"assistant\"] #extract output texts\n",
    "                    total_tokens_input += sum(len(encoder.encode(text, allowed_special=\"all\")) for text in user_texts) #tokenize and count input tokens\n",
    "                    total_tokens_output += sum(len(encoder.encode(text, allowed_special=\"all\")) for text in assistant_texts) #tokenize and count output tokens\n",
    "\n",
    "        print(f\"Total tokens (user): {total_tokens_input}\")\n",
    "        print(f\"Total tokens (assistant): {total_tokens_output}\")\n",
    "\n",
    "        fertility_input = total_tokens_input / total_words_input #fertility calculation input\n",
    "        fertility_output = total_tokens_output / total_words_output #fertility calculation output\n",
    "\n",
    "        result_row = {\n",
    "            \"model\": openai_model,\n",
    "            \"total_words_input\": total_words_input,\n",
    "            \"total_tokens_input\": total_tokens_input,\n",
    "            \"fertility_input\": round(fertility_input, 2), #only 2 decimal places\n",
    "            \"total_words_output\": total_words_output,\n",
    "            \"total_tokens_output\": total_tokens_output,\n",
    "            \"fertility_output\": round(fertility_output, 2) #only 2 decimal places\n",
    "        }\n",
    "\n",
    "        csv_filename = f\"evaluation_fertility_{corpus}.csv\" \n",
    "        file_exists = os.path.exists(csv_filename)\n",
    "\n",
    "        with open(csv_filename, mode='a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result_row.keys())\n",
    "            if not file_exists: #if the file did not exist before, the header needs to be written\n",
    "                writer.writeheader()\n",
    "            writer.writerow(result_row) #save results\n",
    "\n",
    "    elif corpus == \"c4_allenai\": #if the selected corpus is the training dataset\n",
    "        #initialize counter\n",
    "        total_tokens = 0\n",
    "\n",
    "        for example in tqdm(validation_dataset, desc=f\"Tokenizing C4 for {openai_model}\"): #tokenize text to text\n",
    "            text = example[\"text\"]\n",
    "            tokens = encoder.encode(text, disallowed_special=())\n",
    "            total_tokens += len(tokens)\n",
    "\n",
    "        print(f\"Total tokens (C4): {total_tokens}\")\n",
    "\n",
    "        fertility = total_tokens / total_words #fertility calculation c4\n",
    "        \n",
    "        result_row = {\n",
    "            \"model\": openai_model,\n",
    "            \"total_words\": total_words,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"fertility\": round(fertility, 2) #only 2 decimal places\n",
    "        }\n",
    "\n",
    "        csv_filename = f\"evaluation_fertility_{corpus}.csv\" \n",
    "        file_exists = os.path.exists(csv_filename)\n",
    "\n",
    "        with open(csv_filename, mode='a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result_row.keys()) \n",
    "            if not file_exists: #if the file did not exist before, the header needs to be written\n",
    "                writer.writeheader()\n",
    "            writer.writerow(result_row) #save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fertility calculation for HF models tokenizers:\n",
    "for model_name in hf_models: #repeat process for each model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) #load the tokenizer\n",
    "\n",
    "    if corpus == \"lmsys\": #if the selected corpus is the conversational one\n",
    "        #initialize counters\n",
    "        total_tokens_input = 0 \n",
    "        total_tokens_output = 0\n",
    "\n",
    "        for conversation in tqdm(dataset[\"train\"], desc=f\"Tokenizing LMSYS with {model_name}\"):\n",
    "            if conversation.get(\"language\") == \"English\" and \"conversation\" in conversation:\n",
    "                user_texts = [entry[\"content\"] for entry in conversation[\"conversation\"] if entry[\"role\"] == \"user\"] #extract input texts\n",
    "                assistant_texts = [entry[\"content\"] for entry in conversation[\"conversation\"] if entry[\"role\"] == \"assistant\"] #extract input texts\n",
    "                total_tokens_input += sum(len(tokenizer(text, padding=False, truncation=False)[\"input_ids\"]) for text in user_texts) #tokenize and count input tokens\n",
    "                total_tokens_output += sum(len(tokenizer(text, padding=False, truncation=False)[\"input_ids\"]) for text in assistant_texts) #tokenize and count output tokens\n",
    "\n",
    "        fertility_input = total_tokens_input / total_words_input #fertility calculation input\n",
    "        fertility_output = total_tokens_output / total_words_output #fertility calculation output\n",
    "\n",
    "        result_row = {\n",
    "            \"model\": model_name,\n",
    "            \"total_words_input\": total_words_input,\n",
    "            \"total_tokens_input\": total_tokens_input,\n",
    "            \"fertility_input\": round(fertility_input, 2), #only 2 decimal places\n",
    "            \"total_words_output\": total_words_output,\n",
    "            \"total_tokens_output\": total_tokens_output,\n",
    "            \"fertility_output\": round(fertility_output, 2) #only 2 decimal places\n",
    "        }\n",
    "\n",
    "        csv_filename = f\"evaluation_fertility_{corpus}.csv\"\n",
    "        file_exists = os.path.exists(csv_filename)\n",
    "        with open(csv_filename, mode='a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result_row.keys())\n",
    "            if not file_exists: #if the file did not exist before, the header needs to be written\n",
    "                writer.writeheader()\n",
    "            writer.writerow(result_row) #save results\n",
    "\n",
    "    elif corpus == \"c4_allenai\": #if the selected corpus is the training dataset\n",
    "        #initialize counter\n",
    "        total_tokens = 0\n",
    "\n",
    "        for example in tqdm(validation_dataset, desc=f\"Tokenizing C4 with {model_name}\"): #tokenize text to text\n",
    "            text = example[\"text\"]\n",
    "            tokens = tokenizer(text, padding=False, truncation=False)[\"input_ids\"]\n",
    "            total_tokens += len(tokens)\n",
    "\n",
    "        fertility = total_tokens / total_words #fertility calculation c4\n",
    "\n",
    "        result_row = {\n",
    "            \"model\": model_name,\n",
    "            \"total_words\": total_words,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"fertility\": round(fertility, 2)\n",
    "        }\n",
    "\n",
    "        csv_filename = f\"evaluation_fertility_{corpus}.csv\"\n",
    "        file_exists = os.path.exists(csv_filename)\n",
    "        with open(csv_filename, mode='a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result_row.keys())\n",
    "            if not file_exists: #if the file did not exist before, the header needs to be written\n",
    "                writer.writeheader()\n",
    "            writer.writerow(result_row) #save results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
