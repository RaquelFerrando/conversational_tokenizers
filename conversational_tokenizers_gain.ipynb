{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Sequence\n",
    "from tokenizers.pre_tokenizers import Split, Metaspace\n",
    "\n",
    "#RETRAIN TOKENIZERS FOR CONVERSATIONAL PURPOSES\n",
    "\n",
    "#Hugging Face login\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "#parameters for experiment:\n",
    "\n",
    "model = \"mistralai/Mistral-7B-v0.1\" #a different model can be processed in each execution of the code, these ones or others:\n",
    "#\"mistralai/Mistral-7B-v0.1\"\n",
    "#\"meta-llama/Llama-3.1-8B\"\n",
    "#\"google/gemma-2-9b\"\n",
    "#\"deepseek-ai/DeepSeek-R1\"\n",
    "#\"bigscience/bloom\"\n",
    "#\"microsoft/phi\"\n",
    "\n",
    "mode_training = \"output\" #\"input\", \"output\", \"conversation\" are the three possible options\n",
    "batch_size = 100 #to process the corpus\n",
    "\n",
    "#extract the model name after the slash:\n",
    "model_name = model.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the train and test splits:\n",
    "train_corpus = Dataset.from_parquet(\"train_randomsplit.parquet\") #generated with create_corpus.ipynb\n",
    "test_corpus = Dataset.from_parquet(\"test_randomsplit.parquet\") #generated with create_corpus.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to avoid loading all the texts into memory:\n",
    "def batch_iterator(dataset, batch_size=1000, mode = \"conversation\"): #used for training\n",
    "    column_map = {\n",
    "    \"input\": \"clean_input\",\n",
    "    \"output\": \"clean_output\", \n",
    "    \"conversation\": \"clean_conversation\"\n",
    "    }\n",
    "    column = column_map[mode] #depending on the training mode, one of these three columns is used\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc = \"Traning Progress\"):\n",
    "        yield dataset[i : i + batch_size][column] #batches of texts to train the tokenizer\n",
    "\n",
    "\n",
    "#to test the number of tokens generated for input and output separately:\n",
    "def  get_test_corpus_input(test_dataset): #returns batches of the input of the conversations\n",
    "    column = \"clean_input\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "\n",
    "def  get_test_corpus_output(test_dataset): #returns batches of the output of the conversations\n",
    "    column = \"clean_output\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "    \n",
    "#to count the number of tokens in a corpus from a generator (like get_test_corpus_input(test_dataset) or get_test_corpus_output(test_dataset)): \n",
    "def tokenize_and_count_from_generator(generator, tokenizer):\n",
    "    total_tokens = 0\n",
    "    for fragment_list in generator: \n",
    "        batch_tokenized = tokenizer(\n",
    "            fragment_list,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        total_tokens += sum(len(ids) for ids in batch_tokenized[\"input_ids\"]) #numerical identifiers of the tokens\n",
    "    return total_tokens\n",
    "\n",
    "#to organise the different tokenizers that are going to be generated:\n",
    "def create_filename(base_name, mode):\n",
    "    mode_map = {\n",
    "        \"input\": \"only_input\",\n",
    "        \"output\": \"only_output\",\n",
    "        \"conversation\": \"conversation\"\n",
    "    }\n",
    "\n",
    "    base_name += f\"_{mode_map[mode]}\"\n",
    "    base_name += \".csv\"\n",
    "    return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processing: {model}\")\n",
    "\n",
    "#load the tokenizer from the model:\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training of the new tokenizer using the iterator from the training corpus:\n",
    "if model == \"google/gemma-2-9b\" or model == \"mistralai/Mistral-7B-v0.1\":\n",
    "#for these models, the configuration of the tokenizers needs to be changed\n",
    "    backend_tokenizer = original_tokenizer.backend_tokenizer #access the internal tokenizer\n",
    "    #modify normalizer and pretokenizer\n",
    "    backend_tokenizer.normalizer = Sequence(normalizers=[]) \n",
    "    backend_tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "        pre_tokenizers.Whitespace(),  #divides by words\n",
    "        Metaspace(replacement=\"▁\")   #adds '▁' at the start of each word\n",
    "    ])\n",
    "    \n",
    "    #create the directory\n",
    "    os.makedirs(f\"tokenizer_mod_{model_name}\", exist_ok=True)\n",
    "\n",
    "    #save the new configuration\n",
    "    backend_tokenizer.save(f\"tokenizer_mod_{model_name}/tokenizer.json\")\n",
    "    original_tokenizer_mod = PreTrainedTokenizerFast(tokenizer_file=f\"tokenizer_mod_{model_name}/tokenizer.json\")\n",
    "    #retrain tokenizer\n",
    "    tokenizer = original_tokenizer_mod.train_new_from_iterator(batch_iterator(train_corpus, batch_size, mode_training), original_tokenizer.vocab_size, length=len(train_corpus))\n",
    "\n",
    "else:\n",
    "    #retrain tokenizer\n",
    "    tokenizer = original_tokenizer.train_new_from_iterator(batch_iterator(train_corpus, batch_size, mode_training), original_tokenizer.vocab_size, length=len(train_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the output directory path inside the folder 'retrained_conversational_tokenizers':\n",
    "output_dir = os.path.join(\"retrained_conversational_tokenizers\", f\"tokenizer_{model_name}_{mode_training}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#save the tokenizer:\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Retrained tokenizer completed and saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original tokenizer again (the change of configuration in gemma and mistral affects the current copy):\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of tokens for the input of the test corpus for the retrained tokenizer\n",
    "tokens_new_input = tokenize_and_count_from_generator(get_test_corpus_input(test_corpus), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of tokens for the input of the test corpus for the original tokenizer\n",
    "tokens_old_input = tokenize_and_count_from_generator(get_test_corpus_input(test_corpus), original_tokenizer)\n",
    "\n",
    "#compute the gain achieved by the retraining for the input (user) texts\n",
    "gain_input = (1 - tokens_new_input / tokens_old_input) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of tokens for the output of the test corpus for the retrained tokenizer\n",
    "tokens_new_output = tokenize_and_count_from_generator(get_test_corpus_output(test_corpus), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of tokens for the input of the test corpus for the original tokenizer\n",
    "tokens_old_output = tokenize_and_count_from_generator(get_test_corpus_output(test_corpus), original_tokenizer)\n",
    "#compute the gain achieved by the retraining for the output (assistant) texts\n",
    "gain_output = (1 - tokens_new_output / tokens_old_output) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print results\n",
    "print(f\"Model: {model}, Tokens old input: {tokens_old_input}, Tokens new input: {tokens_new_input}, Gain input: {gain_input:.2f}%, Tokens old output: {tokens_old_output}, Tokens new output: {tokens_new_output}, Gain output: {gain_output:.2f}% \")\n",
    "\n",
    "#create the CSV file name\n",
    "csv_file = create_filename(\"conversational_tokenizers\", mode_training)\n",
    "\n",
    "#check if the file already exists\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "#open the file in \"append\" mode if it exists, or \"write\" mode if it doesn't\n",
    "with open(csv_file, mode=\"a\" if file_exists else \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        \"model\", \n",
    "        \"tokens_old_input\", \"tokens_new_input\", \"gain_input\",\n",
    "        \"tokens_old_output\", \"tokens_new_output\", \"gain_output\"\n",
    "    ])\n",
    "    #if the file doesn't exist, write the header\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    #directly write the single result row\n",
    "    writer.writerow({\n",
    "        \"model\": model,\n",
    "        \"tokens_old_input\": tokens_old_input,\n",
    "        \"tokens_new_input\": tokens_new_input,\n",
    "        \"gain_input\": round(gain_input, 2),  #round the gain to 2 decimal places\n",
    "        \"tokens_old_output\": tokens_old_output,\n",
    "        \"tokens_new_output\": tokens_new_output,\n",
    "        \"gain_output\": round(gain_output, 2),  #round the gain to 2 decimal places        \n",
    "    })\n",
    "\n",
    "print(f\"Results saved in {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
