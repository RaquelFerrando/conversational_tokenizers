{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#STUDY OPTIMIZATION OF RETRAINED TOKENIZERS FOR DIFFERENT LANGUAGES\n",
    "\n",
    "#Hugging Face login\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "model = \"mistralai/Mistral-7B-v0.1\" #a different model can be processed in each execution of the code, these ones or others:\n",
    "#\"mistralai/Mistral-7B-v0.1\"\n",
    "#\"meta-llama/Llama-3.1-8B\"\n",
    "#\"google/gemma-2-9b\"\n",
    "#\"deepseek-ai/DeepSeek-R1\"\n",
    "#\"bigscience/bloom\"\n",
    "#\"microsoft/phi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test split of the conversational dataset:\n",
    "test_corpus = Dataset.from_parquet(\"test_randomsplit.parquet\") #generated with create_corpus.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph with the language distribution:\n",
    "\n",
    "#count languages\n",
    "language_counts = Counter(test_corpus['language'])\n",
    "\n",
    "#sort by frequency\n",
    "sorted_languages = language_counts.most_common()\n",
    "languages, counts = zip(*sorted_languages)\n",
    "\n",
    "#compute percentages\n",
    "total = sum(counts)\n",
    "percentages = [count / total * 100 for count in counts]\n",
    "\n",
    "#group into top n + others\n",
    "top_n = 10\n",
    "others_percentage = sum(percentages[top_n:])\n",
    "languages_modified = list(languages[:top_n]) + [\"others\"]\n",
    "percentages_modified = percentages[:top_n] + [others_percentage]\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "colors = plt.cm.get_cmap(\"tab20\", len(languages_modified)).colors\n",
    "\n",
    "plt.pie(\n",
    "    percentages_modified,\n",
    "    labels=languages_modified,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=180,\n",
    "    labeldistance=1.05,\n",
    "    pctdistance=0.85,\n",
    "    colors=colors,\n",
    "    textprops={'fontsize': 19}\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to access the different tokenizers created in \"conversational_tokenizers_gain.ipynb\"\n",
    "\n",
    "model_name = model.split(\"/\")[-1]  #extract the model name after the slash\n",
    "\n",
    "#load tokenizers manually from the conversational_tokenizers folder\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True) #original tokenizer: load from Hugging Face\n",
    "tokenizer_input = AutoTokenizer.from_pretrained(f\"retrained_conversational_tokenizers/tokenizer_{model_name}_input\")\n",
    "tokenizer_output = AutoTokenizer.from_pretrained(f\"retrained_conversational_tokenizers/tokenizer_{model_name}_output\")\n",
    "tokenizer_conversation = AutoTokenizer.from_pretrained(f\"retrained_conversational_tokenizers/tokenizer_{model_name}_conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to test the number of tokens generated for input and output separately:\n",
    "def  get_test_corpus_input(test_dataset): #returns batches of the input of the conversations\n",
    "    column = \"clean_input\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "\n",
    "def  get_test_corpus_output(test_dataset): #returns batches of the output of the conversations\n",
    "    column = \"clean_output\"\n",
    "    return (\n",
    "         test_dataset[i : i + 100][column] \n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "\n",
    "#to count the number of tokens in a corpus from a generator (like get_test_corpus_input(test_dataset) or get_test_corpus_output(test_dataset)): \n",
    "def tokenize_and_count_from_generator(generator, tokenizer):\n",
    "    total_tokens = 0\n",
    "    for fragment_list in generator: \n",
    "        batch_tokenized = tokenizer(\n",
    "            fragment_list,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        total_tokens += sum(len(ids) for ids in batch_tokenized[\"input_ids\"]) #numerical identifiers of the tokens\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check if latin is actually latin in the conversations:\n",
    "\n",
    "def get_test_corpus_conversation(test_dataset):\n",
    "    column = \"conversation\"\n",
    "    return (\n",
    "        test_dataset[i : i + 100][column]\n",
    "        for i in range(0, len(test_dataset), 100)\n",
    "    )\n",
    "\n",
    "latin_conversations = []\n",
    "generator = get_test_corpus_conversation(test_corpus)\n",
    "\n",
    "#loop through batches of 100\n",
    "for i, chunk in enumerate(generator):\n",
    "    #track the start and end index of the current batch\n",
    "    start_idx = i * 100\n",
    "    end_idx = min(start_idx + 100, len(test_corpus))\n",
    "\n",
    "    #get the languages of the current batch\n",
    "    languages = test_corpus[start_idx:end_idx][\"language\"]\n",
    "\n",
    "    for convo, lang in zip(chunk, languages):\n",
    "        if lang == \"Latin\":\n",
    "            latin_conversations.append(convo)\n",
    "            if len(latin_conversations) == 100:\n",
    "                break\n",
    "    if len(latin_conversations) == 100:\n",
    "        break\n",
    "\n",
    "#save the 100 conversations to a file\n",
    "with open(\"latin_conversations.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, convo in enumerate(latin_conversations, 1):\n",
    "        f.write(f\"--- Conversation {i} ---\\n{convo}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrained_tokenizers = { #to iterate through the three tokenizers\n",
    "    \"input\": tokenizer_input,\n",
    "    \"output\": tokenizer_output,\n",
    "    \"conversation\": tokenizer_conversation,\n",
    "}\n",
    "\n",
    "#dictionary to store temporary results for each tokenizer type\n",
    "all_token_counts = {kind: {\n",
    "    \"original_input\": defaultdict(int),\n",
    "    \"original_output\": defaultdict(int),\n",
    "    \"new_input\": defaultdict(int),\n",
    "    \"new_output\": defaultdict(int),\n",
    "} for kind in retrained_tokenizers}\n",
    "\n",
    "#process in batches\n",
    "batch_size = 100\n",
    "for i in range(0, len(test_corpus), batch_size):\n",
    "    batch = test_corpus[i:i+batch_size]\n",
    "    inputs = batch[\"clean_input\"]\n",
    "    outputs = batch[\"clean_output\"]\n",
    "    languages = batch[\"language\"]\n",
    "\n",
    "    #original: reference to compare the performance of the new tokenizers\n",
    "    orig_in = original_tokenizer(inputs, truncation=True, add_special_tokens=False)[\"input_ids\"] #tokenize input texts\n",
    "    orig_out = original_tokenizer(outputs, truncation=True, add_special_tokens=False)[\"input_ids\"] #tokenize output texts\n",
    "\n",
    "    #tokenize with each retrained tokenizer\n",
    "    for kind, tokenizer in retrained_tokenizers.items():\n",
    "        new_in = tokenizer(inputs, truncation=True, add_special_tokens=False)[\"input_ids\"]\n",
    "        new_out = tokenizer(outputs, truncation=True, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        #accumulate token counts\n",
    "        for lang, oi, oo, ni, no in zip(languages, orig_in, orig_out, new_in, new_out): #they all align correctly because they correspond, in order, to the same conversations\n",
    "            all_token_counts[kind][\"original_input\"][lang] += len(oi) \n",
    "            all_token_counts[kind][\"original_output\"][lang] += len(oo)\n",
    "            all_token_counts[kind][\"new_input\"][lang] += len(ni)\n",
    "            all_token_counts[kind][\"new_output\"][lang] += len(no)\n",
    "\n",
    "#function to compute gain\n",
    "gain = lambda orig, new: 100 * (orig - new) / orig if orig > 0 else 0\n",
    "\n",
    "#create and save csvs\n",
    "output_dir = os.path.join(\"gains_by_language\", model_name) #folder to save the results for each tokenizer\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for kind, counts in all_token_counts.items():\n",
    "    results = []\n",
    "    languages = sorted(\n",
    "        counts[\"original_input\"].keys(),\n",
    "        key=lambda l: counts[\"original_input\"][l] + counts[\"original_output\"][l],\n",
    "        reverse=True\n",
    "    )\n",
    "    for lang in languages: #number of tokens (input and output) for the original and the chosen retrained tokenizer\n",
    "        orig_in = counts[\"original_input\"][lang]\n",
    "        orig_out = counts[\"original_output\"][lang]\n",
    "        new_in = counts[\"new_input\"][lang]\n",
    "        new_out = counts[\"new_output\"][lang]\n",
    "\n",
    "        results.append({\n",
    "            \"language\": lang,\n",
    "            \"tokens_old_input\": orig_in,\n",
    "            \"tokens_new_input\": new_in,\n",
    "            \"gain_input\": gain(orig_in, new_in),\n",
    "            \"tokens_old_output\": orig_out,\n",
    "            \"tokens_new_output\": new_out,\n",
    "            \"gain_output\": gain(orig_out, new_out),\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, f\"gain_tokenizer_{kind}.csv\"), index=False) #save results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
