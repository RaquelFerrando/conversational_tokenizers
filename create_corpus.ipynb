{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "#CREATE AND SAVE TRAIN AND TEST SPLITS FROM THE CONVERSATIONAL DATASET LMSYS\n",
    "\n",
    "#Hugging Face login\n",
    "load_dotenv(\"key.env\")\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "dataset = load_dataset('lmsys/lmsys-chat-1m', trust_remote_code=True)\n",
    "\n",
    "#the dataset only has \"train\"\n",
    "dataset[\"train\"].to_parquet(\"train_dataset_lmsys.parquet\") #code to generate train_dataset_lmsys.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset from the Parquet file\n",
    "train_dataset = Dataset.from_parquet(\"train_dataset_lmsys.parquet\")\n",
    "\n",
    "# 80% train 20% test\n",
    "split = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "#obtain train and test splits\n",
    "combined_train = split['train']\n",
    "combined_test = split['test']\n",
    "\n",
    "#save results in parquet format\n",
    "combined_train.to_parquet(\"train_combined_random.parquet\")\n",
    "combined_test.to_parquet(\"test_combined_random.parquet\")\n",
    "\n",
    "print(f\"Train set size: {len(combined_train)}\")\n",
    "print(f\"Test set size: {len(combined_test)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_clean_columns(dataset):\n",
    "    \"\"\"\n",
    "    Adds three new columns to the dataset:\n",
    "    -'clean_input': Contains only the texts from the 'user' role in each conversation.\n",
    "    -'clean_conversation': Contains the texts from both 'user' and 'assistant', merged into a single string.\n",
    "    -'clean_assistant': Contains only the texts from the 'assistant' role in each conversation.\n",
    "    \"\"\"\n",
    "    clean_inputs = [] \n",
    "    clean_conversations = []\n",
    "    clean_outputs = []\n",
    "    \n",
    "    #iterate through each conversation and extract texts from 'user' and 'assistant'\n",
    "    for conversation in dataset[\"conversation\"]:\n",
    "        user_texts = [entry[\"content\"] for entry in conversation if entry[\"role\"] == \"user\"]\n",
    "        assistant_texts = [entry[\"content\"] for entry in conversation if entry[\"role\"] == \"assistant\"]\n",
    "        \n",
    "        clean_inputs.append(\" \".join(user_texts))  #there can be several input texts\n",
    "        clean_outputs.append(\" \".join(assistant_texts)) #there can be several output texts\n",
    "        clean_conversations.append(\" \".join(user_texts + assistant_texts)) #the order of the conversation is not mantained (doesn't matter)\n",
    "    \n",
    "    #add the new columns to the dataset\n",
    "    dataset = dataset.add_column(\"clean_input\", clean_inputs)\n",
    "    dataset = dataset.add_column(\"clean_conversation\", clean_conversations)\n",
    "    dataset = dataset.add_column(\"clean_output\", clean_outputs)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "#add the columns to the previously generated splits\n",
    "combined_train_clean = add_clean_columns(combined_train)\n",
    "combined_test_clean = add_clean_columns(combined_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_clean.to_parquet(\"train_randomsplit.parquet\")\n",
    "combined_test_clean.to_parquet(\"test_randomsplit.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
